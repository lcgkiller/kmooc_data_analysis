{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 분석\n",
    "\n",
    "텍스트(비정형 데이터)로부터 정보를 추출해 내는 작업. 자연어 처리(NLP, Natural Language Processing)나 문서 처리 기술을 이용해서 정보를 추출을 해내는 작업이 먼저 선행이 된다.\n",
    "\n",
    "우선, 전처리 과정이 필요하다\n",
    "[전처리 과정]\n",
    "  - 텍스트 정규화 (Text Normalization)\n",
    "  - 텍스트 정규화란, 텍스트의 형태를 일관되게 변형하는 작업을 의미한다.\n",
    "      1. 토큰화(Tokenization) : 텍스트를 의미 단위(토큰)로 분할하는 작업\n",
    "      2. 어간 추출(Stemming) : 형태가 변현된 단어로부터 어간을 분리하는 작업 등이 포함된다.\n",
    "  - 형태소 분석(POS-Tagging)\n",
    "      1. 토큰으로 잘라낸 단어의 품사 정보들을 추출을 해내는 작업\n",
    "      \n",
    "이후, 텍스트 분석이 본격적으로 진행된다.\n",
    "\n",
    "#### 텍스트 분석의 종류\n",
    "\n",
    "    1. 정보 추출(Information Retrieval)\n",
    "        - 비정형 데이터인 문서(구조화 되지 않는 문서 군집 내에서) 안에서 필요한 정보를 포함하고 있는 정형 데이터의 형태의 정보를 추출해내는 작업\n",
    "    2. 문서 분류(Text Classification)\n",
    "        - 문서들을 특정 분류 체계에 따라 분류하는 작업\n",
    "        - EX) 뉴스가 정치 카테고리에 속하는지, 경제 카테고리에 속하는지 등과 같은 카테고리를 분류하는 작업\n",
    "    3. 감성 분석(Sentiment Analysis)\n",
    "        - 문서에 내포되어 있는 감정과 의견을 추출해내는 작업\n",
    "\n",
    "# 토큰화\n",
    "토큰화(tokenization) : 텍스트를 의미 단위로 분할하는 작업. 이렇게 분할한 결과물 단위를 토큰(token)이라고 함.\n",
    "\n",
    "Token : 텍스트 내부의 원소 (주어진 문장을 공백을 기준으로 단순 분할)\n",
    "Type : 추출된 토큰들 중에서 중복을 제외하고 (text), 어간이 같은 단어의 중복을 제외(had)\n",
    "\n",
    "[언어별 토큰화 방법]\n",
    " - 미국 : 단순 공백 기준 분할\n",
    " - 독일 : 복합명사에 대한 분리 방법이 요구됨\n",
    " - 중국 : 공백이 없으며 여러 문자로 한 단어가 이루어짐. 평균 2.4개의 문자들로 한 개의 단어가 구성\n",
    " - 한국 : 단순 공백 기준이 아닌 '품사' 기준으로 토큰화. 영어보다 조금 더 세밀한 작업들이 필요하다.\n",
    "     - 한국어를 토큰화하는 예시 문장입니다 -> 한국어, 를, 토큰화, 하다, 예시, 문장, 입, 니다\n",
    "     \n",
    "# 어간 추출(stemming)\n",
    "토큰화를 진행한 뒤에는 정확한 의미 파악을 위해서 어간 추출을 진행한다. 여기서 문장 속에 다양한 형태로 변형된 단어의 표제어(lemma)를 찾는  Lemmatization이라는 작업이 진행된다. \n",
    "    - 표제어 : 단어의 의미를 사전에서 찾을 때 사용하는 기본형 \n",
    "![스크린샷, 2017-11-19 23-16-41](https://i.imgur.com/swbGIUj.png)\n",
    "   - Stemming : 단어의 의미를 이해하지 않는다.\n",
    "   - Lemmatization : 날다(동사)인지, 파리(명사)인지 파악\n",
    "   \n",
    "의미를 파악한 후, 어간을 추출한다. 즉, 형태가 변형된 단어에 대해 접사 등을 제거하고 어간을 분리해 내는 작업이다.\n",
    "EX1) fly, flying, flies -> fly\n",
    "EX2) stemming, stemmed, stemmer -> stem\n",
    "    \n",
    "`영어`의 어간 추출하는 대표적인 알고리즘은 `포터 알고리즘`이다.\n",
    "\n",
    "\n",
    "## 포터 알고리즘\n",
    "\n",
    "![스크린샷, 2017-11-19 23-20-16](https://i.imgur.com/I8JRwFO.png)\n",
    "\n",
    "\n",
    "1. Step 1a\n",
    "    - 어간을 추출하기 위해 명사의 복수형을 단수형으로 고치기 위한 규칙을 적용\n",
    "    - 명사의 복수형을 단수형으로 고친다.\n",
    "    - 추상명사는 단수와 복수의 구분이 없기 때문에 변화가 없다(caress->caress)\n",
    "    - 복수형을 만들기 위해서 흔히 사용하는 s는 생략해 단수형으로 만든다(cats->cat)\n",
    "\n",
    "    ![스크린샷, 2017-11-19 23-23-27](https://i.imgur.com/Cw5opG3.png)\n",
    "2. Step 1b, 1b1\n",
    "    - 동사를 원형으로 변환하고, 단어의 품사가 바뀌면서 단어가 변형된 경우에 그 단어의 기본형으로 다시 바꿔준다.\n",
    "    ![스크린샷, 2017-11-19 23-23-54](https://i.imgur.com/BZu2CA6.png)\n",
    "\n",
    "3. Step2\n",
    "    - 단어의 의미의 변형이 일어나는 경우 다시 기본형으로 바꿔준다.\n",
    "\n",
    "# 형태소 분석\n",
    "토큰의 품사 정보를 추출하는 작업. \n",
    "    EX) Jane Plays well with her friends.\n",
    "    -> Jane (NNP), play(VB), well(RB), her(PRP), friends(NNS\n",
    "    \n",
    "## Open Class vs. Closed Class\n",
    "\n",
    "![스크린샷, 2017-11-19 23-28-25](https://i.imgur.com/SUKtM0W.png)\n",
    "Open Class : 명사, 동사, 형용사, 부사 등 새로운 단어의 등장이 상대적으로 많은 품사등을 의미한다.\n",
    "\n",
    "![스크린샷, 2017-11-19 23-27-14](https://i.imgur.com/4kaXyjE.png)\n",
    "Closed Class : 상대적으로 새로운 단어가 잘 추가되지 않는 고정된 세트. 문장 내에서 주로 문법적인 역할을 수행한다.\n",
    "\n",
    "## 형태소 분석기\n",
    "\n",
    "영어 :Standford POS-Tagger, NLTK Pos-Tagger\n",
    "\n",
    "한국어 : Han nanum, Kkma, Komoran, Twitter\n",
    "\n",
    "# 정보 추출\n",
    "구조화 되지 않는 문서 군집 내에서 필요한 정보를 포함하고 있는 문서를 찾아내는 작업 (문서라기 보다는, 구조화된 데이터, 정형 데이터, 즉 테이블 형태로 표현이 될 수 있는 데이터를 의미한다.) 우리가 흔히 인터넷에서 검색을 하는 것도 일종의 정보 추출이라고 할 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "1. 문서 단어 행렬(document term matrix)\n",
    "    ![스크린샷, 2017-11-19 23-31-18](https://i.imgur.com/wSwyzm2.png)\n",
    "    - 이 행렬에서 **1** 은 해당 단어가 해당 문서에 등장한다는 뜻이다. **0**은 해당 단어가 해당 문서에 등장하지 않는다는 뜻이다.\n",
    "    - 각 단어는 벡터로 푠현이 가능하다.\n",
    "    - Circket과 Princess는 포함하되 Pinocchio는 포함하지 않는다는 조건의 벡터 연산이 가능하다. (Cricket AND Princess AND NOT Pinocchio)\n",
    "    - 문서와 단어의 수가 증가하면 **sparse matrix(값의 대부분이 0인 행렬) ** 가 생성된다.\n",
    "    \n",
    "2. 각 단어가 문서에 몇 번 등장하는지 빈도를 보여주는 행렬\n",
    "    ![스크린샷, 2017-11-19 23-34-20](https://i.imgur.com/silzLzF.png)\n",
    "    - 문서는 단어의 등장 빈도를 나타내는 **count vector** 로 표현될 수 있다.\n",
    "    - Bag of Words 모델 : 문서 내에 등장하는 단어들의 순서를 무시한다.\n",
    "        - John is quicker than Mary vs Mary is quicker than John : 이 문장은 같은 벡터를 가지고 있다. 등장하는 단어의 순서는 고려하지 않는다.\n",
    "        \n",
    "3. Term Frequence(TF\n",
    "    - 각 단어가 문서 내에서 정말 중요한 단어인가?\n",
    "    - 단어 빈도 tft, d는 특정 단어 t가 특정 문서 d 내에 등장하는 빈도를 의미한다.\n",
    "        - 특정단어가 10번 등장하는(tf=10) 문서가 1번 등장하는 **(tf=1) 문서보다 더 많이 타당**하다고 볼 수 없다.\n",
    "        - tf값이 크다면 이 단어는 특정 문서에 더 많이 등장한다는 뜻이다. 하지만 특정 단어가 10번 등장하는 문서가 1번 밖에 등장하지 않는 문서보다 더 적합하거나 타당하다고 보기 어렵다.\n",
    "        - 예를 들어서, good, increase, high, of, the, a와 같은 단어는 많이 나오더라도 그 문서의 의미를 다른 의미와 구별하기 쉽지 않다.\n",
    "        - 오히려 드물게 등장하는 단어가 하나의 문서를 다른 문서와 더 구별하는 데 유용하게 사용될 수 있다.\n",
    "        - `Back-propagation`(즉, 역전파)와 같은 단어가 이 문서와 다른 문서와 차별화시키는 좋은 예가 될 수 있다.\n",
    "            - 드문 단어들에 대해 가중치를 부여할 필요성이 있다는 개념을 도입한 용어가 **Inverse Document Frequency(IDF)** 이다.\n",
    "            \n",
    "4. Inverse Document Frequence (IDF)\n",
    "    ![스크린샷, 2017-11-19 23-41-51](https://i.imgur.com/9Mnyy9Y.png)\n",
    "    - df(=document requency) : 단어 t가 등장한 문서의 수\n",
    "    - dft는 단어 **t의 중요도의 역수**와 같다.\n",
    "    - 단어 t의 idf 수치 idft는 t의 **단어의 중요도를 나타내는 척도**\n",
    "    - the와 같은 정관사는 df의 값이 높다. 반대로 Barbossa(캐리비안 해적에 나오는 캐릭터)는 다른 문서에서 나오기 힘들다. 그래서 다른 문서와 구별하는 데 큰 역할을 할 수 있다.\n",
    "    ![스크린샷, 2017-11-19 23-43-16](https://i.imgur.com/IMCxZUS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 분류(Text Classification)\n",
    "문서를 카테고리, 주제, 장르 등의 기준으로 분류하는 방법 EX) 스팸 필터링, 저자 확인, 언어 확인, 감성 분석(sentiment analysis)\n",
    "\n",
    "입력 데이터(Input) \n",
    "   - 문서(document) : d\n",
    "   - 클래스(장르) : C = {c1, c2, ..., cn}\n",
    "   - training set: (d1, c1), (d2, c3), (d3, c3), (d4,cn), ..., (dm, c1)\n",
    " \n",
    "출력 데이터(output)\n",
    "    - 문서 d가 속하는 클래스 c 는  C에 속한다.\n",
    "    \n",
    "규칙(rule-based)에 따른 문서 분류\n",
    "    - 비교적 단순한 규칙에 의해서 문서를 분류하는 방식. 규칙을 만들고 유지 관리하는 비용이 적지 않다.\n",
    "    - EX) 스팸 필터링 : 'dollars'와 'loan'이 같이 등장할 경우 스매 메일로 처리\n",
    "    \n",
    "지도학습 알고리즘을 사용한 문서 분류\n",
    "    - 문서, 클래스, training set을 input으로 넣고, output으로 학습된 문서 분류기(classifier) : 문서 d가 어느 클래스(c)에 들어갈지 알아낼 수 있다.\n",
    "    \n",
    "## 나이브 베이즈(Naive Bayes)를 사용한 문서 분류\n",
    "Bayes' rule : 문서 d가 클래스 c에 속할 확률\n",
    "\n",
    "부류가 여러 개일 경우\n",
    "    - 각각의 분류에 해당하는 조건부 확률 계산\n",
    "    - 입력문서 : 고정되어 있음\n",
    "    - 가장 높은 확률을 가지는 클래스가 해당 문서가 속하는 클래스라고 할 수 있다.\n",
    "    \n",
    "![스크린샷, 2017-11-21 01-51-15](https://i.imgur.com/FNRny5D.png)\n",
    "\n",
    "문서 d가 클래스 c에 속할 확률을 argmax P(c|d)라고 한 다음, 베이즈 정리를 이용해 풀어쓴다.\n",
    "그 다음에 문서 d가 속할 확률이 가장 높은 클래스 c를 찾는 것이 목적이기 때문에 문서 d는 계산 중에 고정되어 있다는 것을 이용해서 생략한다.\n",
    "\n",
    "1. 각 클래스에 단어가 등장하는 빈도수 계산\n",
    "\n",
    "\n",
    "2. 빈도수를 사용해 확률 계산\n",
    "    ![스크린샷, 2017-11-21 01-55-26](https://i.imgur.com/3mB7fSz.png)\n",
    "\n",
    "3. 계산 결과 SF에 속할 확률이 높으므로 문서 d는 SF로 분류\n",
    "  - 이러한 계산 방법은 학습 데이터에 없는 새로운 단어가 있는 문서를 분류하려 하면 모든 계산식이 **0** 이 된다.\n",
    "  - 이러한 문제점을 해결하는 것이 Laplace Smoothing이다\n",
    "    - 확률을 곱하고 있는데 그중에 하나라도 0이 되면 다른 것도 0이 됨\n",
    "    - 모든 빈도에 **+1** 을 해줌으로써 계산을 한다.\n",
    "\n",
    "## 감성분석(sentiment analysis)\n",
    "긍정적인 표출, 부정적인 표출 외에도 행복, 분노, 슬픔과 같은 감정 세분화 분석이 가능하다. 이를 이용해서 시장 움직임이나 투표 결과 등을 예측할 수 있다.\n",
    "\n",
    "크게 세 단계로 감성 분석의 심화 단계를 구분 가능하다.\n",
    "  1단계 : 긍정과 부정 구분\n",
    "  2단계 : 감성의 강도를 1-5의 수치로 추출\n",
    "  3단계 : 특성 감정을 추출하고 감정 발생 원인과 대상을 추출\n",
    "\n",
    "### 나이브 베이즈를 사용한 감성 분석\n",
    "1. 토큰화 진행\n",
    "2. 토큰들의 속성 추출\n",
    "  - 모든 토큰 중에서 감성을 나타내는 **형용사** 를 추출한다.\n",
    "\n",
    "여기서, 문서 분류와의 차이점은 긍정, 중성, 부정으로 구분된다는 것이다.\n",
    "\n",
    "### 파이썬 실습\n",
    "Workflow : 영어 뉴스 데이터 수집 -> 데이터 전처리 -> Word Cloud 형태로 시각화 -> 특징 추출 -> 뉴스 분류\n",
    "\n",
    "텍스트(비정형 데이터)로부터 정보를 추출해내는 작업. (문서 분류, 감정 분석, 정보 추출)\n",
    "\n",
    "### 텍스트 분석과정\n",
    "1. 텍스트 데이터\n",
    "2. 전처리\n",
    "  - 토큰화 : 텍스트를 의미 단위로 분할하는 작업\n",
    "  - 정제 : 무의미한 토큰을 제거하는 작업\n",
    "  - 형태소 분석 : 토큰의 형태소를 파악하는 작업\n",
    "3. 속성 추출 \n",
    "  - 의미 단위가 가지는 수치정보, 가중치 속성 등\n",
    "4. 분류 모델 생성\n",
    "  - 기계학습 알고리즘을 사용한 텍스트 분류 모델 학습\n",
    "5. 분류 결과\n",
    "  - 분류 결과 추출 및 분류기 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "    # http.client : http 프로토콜 관리 패키지\n",
    "    # urllib.request : URL을 열고, 데이터를 읽어들이는 패키지\n",
    "    # urllib.parse : 읽어온 데이터를 문법적으로 분석하는 모듈\n",
    "    # urllib.error : 0 request에서 발생하는 오류를 처리하는 모듈\n",
    "    # base64 : 읽어 온 이진 형태의 데이터를 ASCII 형태로 바꿀 때 사용\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 수집한 뉴스 데이터를 저장할 데이터 르에미 생성\n",
    "df= pd.DataFrame(columns=('name', 'description', 'category')) # 뉴스 제목, 뉴스 요약 내용, 뉴스 카테고리\n",
    "\n",
    "# 5430ca5e40e94aa69b5f8aca3c76c02a\n",
    "# 발급받은 API 키 입력\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key' : '5430ca5e40e94aa69b5f8aca3c76c02a' ,\n",
    "}\n",
    "\n",
    "# 뉴스 수집을 위한 변수 설정\n",
    "params = urllib.parse.urlencode({\n",
    "    'Category' : 'World',\n",
    "    'Market' : 'en-GB',\n",
    "    'Count' : 100\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'gaierror' object has no attribute 'errono'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-39c8e62e8c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPSConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'api.cognitive.microsoft.com'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/bing/v7.0/news/?%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{body} \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 936\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-39c8e62e8c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Error {0} ] {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrono\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'gaierror' object has no attribute 'errono'"
     ]
    }
   ],
   "source": [
    "# try문에서 일련의 작업 시도\n",
    "# http 프로토콜을 사용해 데이터를 요청하는 http.client의 모듈 중 HTTPSCOnnection을 사용해 지정된 URL과 연결\n",
    "# 연결 요청 정보를 URL 뒤 변수로 선언하고, request함수를 사용하여 데이터 요청\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection('api.cognitive.microsoft.com')\n",
    "    conn.request('GET', '/bing/v7.0/news/?%s' % params, \"{body} \", headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    print(data)\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"[Error {0} ] {1}\".format(e.errono, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.decode('utf-8')\n",
    "obj = json.loads(data)\n",
    "val = obj['value']\n",
    "val[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = len(df)\n",
    "for i in range(0, len(val)):\n",
    "    df.loc[l+i] = [val[i]['name'], val[i]['description'], val[i]['category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sh = df.iloc[np.random.permutation(len(df))]\n",
    "df_sh.to_csv('bing_news_shuffle_0517.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "기계학습 알고리즘에 데이터를 효과적으로 입력하기 위해서 필요한 아주 중요한 작업이다. 데이터의 특정 속성이 없거나 정보가 누락되어 있는 불완전한 데이터는 필터링을 해주고, 잘못된 값이 있는 경우에는 제거를 해주게 된다. 그리고 데이터에 충돌하는 값과 또는 불일치하는 값 역시 제거를 해야 한다. 이 과정에서 토근화, 정제, 형태소 분석을 하게 된다.\n",
    "  - 토큰화(tokenize) : 텍스트를 단어나 구와 같은 의미 단위로 분할하는 작업\n",
    "  - 정제(cleansing) : 텍스트에 포함된 무의미한 단어, 불용어를 제거하는 작업\n",
    "  - 형태소 분석(pos-tagging) : 토큰의 품사를 파악하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # 영문 자연어 처리 패키지 NLTK(Natural Langugage Toolkit)\n",
    "print(df.ix[0]['description']) # ix함수를 이용해 0번 인덱스 *행*의 description을 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "tokens = nltk.word_tokenize(df.ix[0]['description']) # \"단순 공백\" 기반 토큰화\n",
    "tokens\n",
    "\n",
    "tokens = [token.lower() for token in tokens if len(token) > 1]\n",
    "tokens\n",
    "\n",
    "tokens_bigram = nltk.bigrams(tokens)\n",
    "for token in tokens_bigram:\n",
    "    print(token)\n",
    "    # bi-gram : 단어들을 두 개씩 묶어 하나의 토큰으로 추출한 것\n",
    "    # EX) The shopping center stays open until 9 a.m. => (the, shopping), (shopping, center), (center, stays), ...\n",
    "    \n",
    "tokens_trigram = nltk.trigrams(tokens)\n",
    "for token in tokens_trigram:\n",
    "    print(token)\n",
    "    # trigram : 단어들을 세 개씩 묶어 하나의 토큰으로 추출한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 정제(cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "#정제\n",
    "#######\n",
    "\n",
    "# 텍스트 내의 불용어(stop words)를 제거하는 작업 \n",
    "#                   불용어 : 관사(a, an, the), 지시대명사(this, that 등) 하나의 문서를 다른 문서와 구별하는 데 별로 영향을 미치지 않는 단어\n",
    "# 불용어는 고려 대상에서 제거한 뒤 해당 문장이나 문서로부터 특징을 추출하는 거이 좋다.\n",
    "\n",
    "# EX) The shopping center stays open until 9 a.m. => The, unitll\n",
    "# NLTK 패키지에서는 샘플 corpus(말뭉치, 문서집합) 제공. 단순한 소설이나 뉴스 등의 문서 외에도 쉬운 분석을 위해 제공하는 corpus를 제공한다.\n",
    "# 여기서는 불용어를 담고 있는 stopwords를 사용한다. (덴마크어, 네덜라든어, 핀란드어, 프랑스어 등 다양한 언어의 불용어 제공\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_clean = [ token for token in tokens if not token in stop_words ] # stopwords에 존재하지 않는 경우에 token_clean에 저장\n",
    "tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 품사 추출(POS tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "tokens_tagged = nltk.pos_tag(tokens_clean) # 토큰들의 품사를 추출. 각 토큰과 해당 품사를 변수 tokens_tagged에 저장한다. \n",
    "print(tokens_tagged)\n",
    "    # JJ : 형용사, NN : 명사, VBG : 동사\n",
    "\n",
    "# nltk.help.upenn_tagset()\n",
    "    # tag 의미 확인\n",
    "    # nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_noun = [word for word, pos in tokens_tagged if pos in ['NN', 'NNP']] # NN : 명사, NNP : 고유명사\n",
    "print(tokens_noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # 데이터 시각화를 지원함\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Word Cloud 모듈 생성\n",
    "stopwords = set(STOPWORDS) # STOPWORDS를 set함수를 사용해 변수 stopwords에 저장\n",
    "wc = WordCloud(background_color=\"white\", max_font_size=100, max_words=50, stopwords=stopwords) # 워드클라우드를 그리는 모듈 wc 생성\n",
    "\n",
    "text_data = df['description'].str.cat(sep=',') # cat 함수를 이용해 문자열들을 한 문자열로 연결\n",
    "                                                                # 한 문자열로 합쳐진 뉴스 내용을 text_data에 저장\n",
    "wordcloud = wc.generate(text_data) # generate 함수를 사용해 문자열 text_data에 대해 word cloud를 그려 변수에 저장\n",
    "\n",
    "plt.figure(figsize=(10, 10)) # figure함수를 사용해서 화면에 시각화 할 공간을 설정\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off') # x축과 y축을 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 특징값 추출\n",
    "\n",
    "문서 단어 행렬이란?\n",
    "각 문서와 단어에 해당하는 수치를 표현한 행렬이다. 토큰의 TF-IDF 값을 사용해서 문서 단어 행렬을 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 단어 행렬 구성\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 2. 배열 변환\n",
    "text_data_list = df['description'].astype(str).tolist()\n",
    "text_data_arr = np.array([''.join(text) for text in text_data_list])\n",
    "        # 텍스트 데이터를 astype 함수를 사용해 문자열로, tolist함수를 사용해 리스트로 변환 후, text_data_list 변수에 저장\n",
    "        # Numpy의 array 함수와 for문을 사용해 배열로 변환 후 text_data_arr에 저장\n",
    "        \n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), strip_accents='unicode', norm='l2')\n",
    "        # min_df = 2 : 2번 이상 등장하는 단어\n",
    "        # ngram_range(1, 2)\n",
    "        # strip_accents = 'Unicode' : accents(억양표시)를 제거\n",
    "        # nrom='l2' : pearson 함수를 사용해 정규화(normalization) 진행\n",
    "text_data = vectorizer.fit_transform(text_data_arr)\n",
    "        # vectorizer의 fit_transform 함수를 사용해 배열에 저장된 데이터의 문서단어행렬을 구함\n",
    "        \n",
    "df_tfidf = pd.DataFrame(text_data.A, columns=vectorizer.get_feature_names())\n",
    "df_tfidf\n",
    "    # 행 : 문서번호\n",
    "    # 열 : 토큰들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 뉴스 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix # 분류 결과 건수를 나타냄\n",
    "from sklearn.metrics import classification_report # recall, precision, f-measure를 제공하는 모듈\n",
    "from sklearn.metrics import f1_score # f-measure \n",
    "from sklearn.metrics import accuracy_score # 정확도 수치 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = df['description'].astype(str).tolist() # 뉴스 내용(description)과 카테고리를 각각 리스트형 변수로 변환\n",
    "category = df['category'].astype(str).tolist()\n",
    "\n",
    "trainset_size = int(round(len(description)*0.8)) # 80%는 훈련 세트(training data), 나머지는 테스트 세트로 구성\n",
    "\n",
    "x_train = np.array([''.join(data) for data in description[0:trainset_size]]) # 뉴스 내용(description)의 training set 범위 내 내용 data를 join 함수를 사용해 연결하고 x_train에 저장 \n",
    "y_train = np.array([data for data in category[0:trainset_size]]) # 뉴스 카테고리(category) 데이터를 y_train에 저장\n",
    "\n",
    "x_test = np.array([''.join(data) for data in description[trainset_size+1 : len(description)]])\n",
    "y_test = np.array([data for data in category[trainset_size+1 : len(description)]])\n",
    "\n",
    "\n",
    "# 2. 문서 단어 행렬 구성\n",
    "X_train = vectorizer.fit_transform(x_train) # fit_transform : training set을 기반으로 문서단어행렬 구성\n",
    "X_test = vectorizer.transform(x_test) # transform 함수를 사용해 문서단어행렬 구성\n",
    "\n",
    "df_per = pd.DataFrame(columns=['Classifier', 'F-Measure', 'Accurcacy']) # 각 분류 모델의 성능을 기록할 데이터 프레임 df_per 선언\n",
    "df_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나이브 베이즈 알고리즘을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB().fit(X_train, y_train) # fit함수를 이용해 모델을 학습\n",
    "nb_pred = nb_classifier.predict(X_test) # predict로 test set에 대한 분류, 예측 값을 np_pred에 저장\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, nb_pred))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, nb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm =round(f1_score(y_test, nb_pred, average='weighted'), 2)\n",
    "ac = round(accuracy_score(y_test, nb_pred, normalize=True), 2)\n",
    "df_per.loc[len(df_per)] = ['Naive Bayes', fm, ac]\n",
    "df_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_classifer = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "dt_pred = dt_classifer.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, dt_pred))\n",
    "print(classification_report(y_test, dt_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm =round(f1_score(y_test, nb_pred, average='weighted'), 2)\n",
    "ac = round(accuracy_score(y_test, nb_pred, normalize=True), 2)\n",
    "df_per.loc[len(df_per)] = ['Decision Tree', fm, ac]\n",
    "df_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest 함수 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, rf_pred))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm =round(f1_score(y_test, rf_pred, average='weighted'), 2)\n",
    "        # f1_score : 실제값과 분류 결과값을 비교해서 f_measure 값을 계산한다.\n",
    "        # average = 'weighted'를 사용해 각 클래스마다 가중치 적용 소수점 2번째 자리까지 올림\n",
    "\n",
    "ac = round(accuracy_score(y_test, rf_pred, normalize=True), 2)\n",
    "        # normalize=True를 통해 정확도 출력, 소수점 2번째 자리까지 반올림\n",
    "    \n",
    "df_per.loc[len(df_per)] = ['Random Forest', fm, ac]\n",
    "    # loc함수를 상ㅇ해서 데이터레임에 인덱스를 지정\n",
    "    \n",
    "df_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_classifier = LinearSVC().fit(X_train, y_train) # fit함수를 사용해 모델 훈련\n",
    "svm_pred = svm_classifier.predict(X_test) # test set에 대한 분류, 예측 값을 구한 후 변수 dt_pred에 저장\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, svm_pred))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm =round(f1_score(y_test, nb_pred, average='weighted'), 2)\n",
    "ac = round(accuracy_score(y_test, nb_pred, normalize=True), 2)\n",
    "df_per.loc[len(df_per)] = ['Decision Tree', fm, ac]\n",
    "df_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_1 = df_per.set_index('Classifier') # 시각화를 위해 분류기 명을 set_index 함수를 사용해 index로 설정\n",
    "df_per_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_per_1[['F-Measure', 'Accuracy']].plot(kind='bar', title='Performance', figsize=(10, 7), legend=True, fontsize=12)\n",
    "    # kind='bar' : 막대 그래프\n",
    "    # title=\"performance' : 그래프 제목\n",
    "    # figsize : 그래프 크기\n",
    "    # legend : 데이터 설명\n",
    "    # fontsize : 글씨 크기\n",
    "ax.set_xlabel('Classifier', fontsize=12) # x축 내용 지정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "df = pd.DataFrame(columns=('name', 'description', 'category'))\n",
    "\n",
    "df = pd.read_csv('bing_news_shuffle_0517.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
